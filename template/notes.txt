Gliederung:
    (1. Abstract
        - Arbeit kurz darstellen, was ist interessant, was sind die Ergebnisse
        - Leser anregen weiter zu lesen)
    2. Einleitung
        - Inhalt
            - Begründung der Arbeit
            - Art und Umfang des Problems und Teilprobleme beschreiben
            - Übersicht über die wichtigste Literatur zu Orientierung des Lesers
            - Untersuchungsmethode und Gründe für Wahl der Methode nennen
            - Wichtigste Ergebnisse nennen
            - Grundsätzliche Schlussfolgerungen nennen (bewertet)
            - Am Ende: Übersicht übr Struktur der Arbeit
        - Aufbau
            - Bereich festlegen
                - Zentralität befestigen
                - aktueller Wissensstand
            - Zusammenfassung der bisherigen Forschung
                - Themen in der Literatur skizzieren
            - Aktuelle Forschung vorbereiten
                - Lücke aufzeigen
                - Frage aufwerfen
            - Einführung in aktuelle Forschung
                - Zweck angeben
                - Überblick über derzeitige Forschung
        - Checklist
            - Was ist das allgemeine Problem und warum ist es wichtig?
            - Was ist das spezifische Problem und sollte es den Leser interessieren?
            - Wie unterscheidet sich die Arbeit von früheren Arbeiten?
            - Was sind die Ziele/Beiträge und inwiefern sind die neu?
            - Was sind die wichtigsten Ergebnisse?
            - Was ist der allgemeine Ansatz/Grundriss
    3. Hintergrund / Literaturübersicht
        - spezifische Überschrift angebracht
        - wichtigste Ideen des aktuellen stands der Wissenschaft darzustellen (nicht analysieren)
        - bis hin zu eigenen brillianten Ideen
        - Abschnitte nach Idee und nicht nach Autor gliedern
        - Checklist Related Work:
            - Was machen andere?
            - Wie unterscheidet sich dies von meiner Arbeit? (mein Ansatz besser, Synergien, Kompromiisse)
        - Checklist Hintergrund:
            - Was ist der notwendige Hintergrund um die Arbeit zu verstehen?
            - kann auch sehr lang ausfallen
    4. Problembeschreibung / Aufgabenstellung / Forschunsfrage
    5. Methoden
    6. Präsentation der Daten
    7. Interpretation
    8. Diskussion
    9. Abschluss
    10. Referenzen

- arten von config-optionen
- ML-Librarys Besonderheiten mit Klassen

Die Konfiguration legt beispielsweise fest, wo im lokalen Dateisystem Daten gespeichert werden sollen, an welche Ports sich ein Server binden soll
und sogar welche Algorithmen in verschiedenen Teilen eines großen Systems verwendet werden sollen.
Die Programmkonfiguration wird häufig in Form einer Abbildung von Optionsnamen auf Werte gespeichert.
Einige Konfigurationsmechanismen, wie z.B. die Unix-Systemumgebung, verwenden beliebige Zeichenketten als Option.

1.2
Eingrenzung auf Source Code und Python und Machine Learning Frameworks. Siehe ORPLOcator

2. Hintergrund: statische Code-Analysen, KonfigOption, machinelearning librarys (was siie sind, wifür sie genutzt)
3. related Work ( zusammenfassen, ergebnisse, abgrenzuung zu meiner arbeit)
4. vorgehensweise (methodogology): statische code-analyse, web-scraping (kommt auch in hintergrund, wie alle anderen techniken)
5. evaluation (mit unit tests, testset von machinelearning projekten raussuchen und parsen),
6. diskussiin (bewertung der ergebnisse)
7. fazit (

mACHINE LEARNING algorithmen werden konfigurirer (mit hyperparameter, defaultvalues), es geht um die ml-algorithmen ´,
die initialisert werden udn das passier t in den klassen, iehe config of machine learming in scolar, oder blogpost bei medium,
%Maschinelles Lernen (ML) wird eingesetzt, um Maschinen beizubringen, wie sie Daten effizienter verarbeiten können.
%Manchmal können wir nach der Sichtung der Daten die Informationen, die wir aus den Daten gewinnen, nicht interpretieren.
%In diesem Fall wenden wir maschinelles Lernen an.
%Mit der Fülle der verfügbaren Datensätze steigt auch die Nachfrage nach maschinellem Lernen.
%Viele Branchen setzen maschinelles Lernen ein, um relevante Daten zu extrahieren.
%Der Zweck des maschinellen Lernens besteht darin, aus den Daten zu lernen.
%Es wurden viele Studien darüber durchgeführt, wie Maschinen selbständig lernen können, ohne explizit programmiert zu werden.
%Viele Mathematiker und Programmierer wenden verschiedene Ansätze an, um eine Lösung für dieses Problem zu finden, das riesige Datensätze umfasst.

%\subsection{Ursprung}
%Der Ursprung des maschinellen Lernens liegt bereits in der Mitte des 20. Jahrhunderts, als der Psychologoe Frank Rosenblatt
%von seiner Arbeitsgruppe eine Maschine zur Erkennung von Buchstaben des Alphabets bauen ließ \cite[S. 1385]{FRADKOV20201385}.
%Das Konzept der Maschine basiert auf der Verarbeitung von Signalen analog zum menschliche Nervensystem, weshalb sie
%als Prototyp der modernen künstlichen neuronalen Netze gilt \cite[S. 1385]{FRADKOV20201385}.
%Der kommerzielle Durchbruch ließ dennoch bis auf den Anfang des 21. Jahrhunderts auf sich warten.
%Nach \citeauthor{FRADKOV20201385} ist dies auf drei Trends zurückzuführen, die zusammen einen spürbaren Synergieeffekt bewirkten \cite[S. 1387]{FRADKOV20201385}. \\

%Unter anderem durch den Erfolg und Möglichkeiten des Internets ist nicht nur das Vorhandensein von Daten enorm gewachsen,
%auch ihre Heterogenität führt dazu, dass herkömmliche Methode zur Verarbeitung und Informationsgewinnung nicht mehr ausreichen \cite[S. 1387]{FRADKOV20201385}.
%Durch das Aufkommen von \textit{Big Data} werden neue Ansätze des maschinellen Lernens nicht nur aus dem Motiv des wissenschaftlichen Erkenntnisgewinns entwickelt,
%sondern auch aufgrund der praktischen und kommerziellen Notwendigkeit, die rasant wachsenden generierten Datenmengen leistungsfähig verarbeiten zu können. \\

%Ein weiterer entscheidender Faktor ist die Senkung der Kosten für parallele Berechnung und Speicher.
%Dies umfasst die Weiterentwicklung und Verwendung von Grafikprozessoren (vor allem von Nvidia),
%die zu erheblichen Verbesserungen der Rechenleistung führen \cite[S. 1387]{FRADKOV20201385}.
%Gleichzeitig sinken die Kosten für Arbeitsspeicher stetig, was die Verarbeitung großer Datenmengen zusätzlich erleichtert \cite[S. 1387]{FRADKOV20201385}.
%Aus Softwaresicht kommt schließlich die MapReduce-Technologie von Google bzw. später auch Hadoop hinzu, die es ermöglicht,
%komplexe Berechnungen auf mehrere Prozessoren zu verteilen\cite[S. 1387]{FRADKOV20201385}. \\

%Der dritte Trend ist die Entwicklung künstlicher neuronaler Netze, die um ein Vielzahl an Zwischenschichten erweitert wurden
%und so noch komplexere Berechnungen ausführen können.
%In diesem Zusammenhang spricht man auch von \textit{Deep Learning}. \\

%\subsection{Algorithmische Ansätze}
%Im Allgemeinen werden die Ansätze des maschinellen Lernens in drei große Kategorien unterteilt, je nach Art des \textit{Signals}
%oder \textit{Feedbacks}, das dem lernenden System zur Verfügung steht \cite[S. 2]{FRADKOV20201385}:
%\begin{itemize}
% \item Überwachtes Lernen (supervised learning)
% \item Unüberwachtes Lernen (unsupervised learning)
% \item Bestärkendes Lernen (reinforcement learning)
%\end{itemize}

%Das \textit{überwachte Lernen} erfordert ein Training mit gelabelten Daten, die Eingaben und gewünschte Ausgaben haben \cite[S. 2]{cite-key}.
%So weiß das Modell zum Beispiel, ob auf einem bestimmten Foto ein Hund abgebildet ist oder wie viel ein bestimmtes Haus kostet.
%Auf dieser Grundlage kann es dann so trainiert werden, dass es neue Hunde erkennt oder den Preis neuer Häuser schätzt. \\

%Im Gegensatz dazu erfordert das \textit{unüberwachte Lernen} keine markierten Trainingsdaten und erhält lediglich Eingabedaten \cite[S. 2]{cite-key}.
%Die richtige Antwort ist entweder nicht bekannt oder existiert nicht.
%Stattdessen sucht das Modell nach sinnvollen Strukturen in den Daten, um neue Erkenntnisse über ein Thema zu gewinnen \cite[S. 383]{mahesh2020machine}.
%Unüberwachtes Lernen kann zum Beispiel dazu verwendet werden, in einem Online-Store Kunden zu finden, die einen ähnlichen Geschmack haben,
%und Artikel empfehlen, die diese Kunden gekauft haben. \\

%Das \textit{bestärkende Lernen} hingegen unterscheidet sich sehr deutlich von den beiden vorherigen Kategorien.
%Das Modell ist hier ein aktiver Akteur, das mit seiner externen Umgebung interagiert
%und positives oder negatives Feedback für seine Handlungen erhält \cite[S. 2]{cite-key}.
%Aus diesen Rückmeldungen erlernt es optimale Handlungsstrategien für seine Umgebung zu entwickeln \cite[S. 384]{mahesh2020machine}.
%Solche Modelle können zum Beispiel für selbstfahrende Autos verwendet werden oder um einer Maschine Gesellschaftsspiele beizubringen.\\



belege für komplexität erhöhung und so

Problemverdeutlichung: alte Tensorflowklassen implementation vs. neue

orp locator 1. seite für code
orp locator 194 für related works

Das Extrahieren von KonÞgurationswerten ermöglicht eine statische Prüfung auf diese Art von Fehlern und kann dem Benutzer Stunden an Zeit ersparen, wenn ein falscher Konfigurationswert einen langen Batch-Job zum Scheitern bringt

--> Evaluation: welche konfigwerte am häufigsten auftauchen

Datenfluss:
- On Top Datenflussanalyse draufsetzen
- Auf File Ebene
-
%https://www.mattlayman.com/blog/2018/decipher-python-ast/


Scraping:
Es gibt zwei wesentliche Module eines Web-Scraping-Programms - ein Modul für die Erstellung einer HTTP-Anfrage, z. B. Urllib2 oder Selenium, und ein weiteres für das Parsen und Extrahieren von Informationen aus rohem HTML-Code, z. B. Beautiful Soup oder Pyquery.

Gliederung Methodik-Teil
- Python Libraries

Evaluation:
- es gibt kein ground truth also keine Daten die sich mit ML-Libraries beschftigt habe nund mit denen man es abgeleichen kann.
- Daher wird sich an anderen Papern orientiert, die es auc hso gemacht haben zum Beispiel ...

arbitrary argument list
arbitrary keyword argument dictionary

Probleme

x = 1
x = 2
ergibt 2 mal x = 2
ausblick: verbesserungen das jede Variable erkannt wird, z.B. var = x + 2

verbesserungen: Bei Listen, nach Appends suchen
vererbung
